{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a440f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b01e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a8f56a3f8e448c997f806029abdadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4c22d0bc6d4cdd884542d9c57ffe89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe797dbde6c4345a5e6d5b3e616063f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883484eb2b15487781b1366b08c1c483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61692dc7fc94335a0597ea8b080c03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ae6be41f834e8e99fc94c92cea652c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc673f7555b14554b85fc042cc10ba5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1e5586ea7b46259d4b51df0603ae3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f071260c1a6c44e18355f816ffe44622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03284d52a9dd4067a123fc7dab420a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2daba5be387e423aaf032a8ab59ad635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2e315495f548b49a67f40438b81c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c0473382c041a99ae25c6b5553230c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pick your model\n",
    "model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "# load model + processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9549ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "<image>\n",
      "You are an InDrive car condition reviewer. Respond in a short, friendly sentence of up to 15 words. Only perfectly clean and undamaged cars are allowed. If the car is perfect, say something positive and end with 'Good for customers'. If the car has any issue, respond with 'Not acceptable (reason)', where reason is a brief description of the main problem (e.g., dirty, dent, rust). Always make the response polite and clear.\n",
      "user\n",
      "\n",
      "assistant\n",
      "Good for customers. The car is clean and undamaged, with a well-maintained exterior.\n"
     ]
    }
   ],
   "source": [
    "# load example image\n",
    "image = Image.open(r\"C:\\Users\\User\\Downloads\\kiario_good.jpg\")\n",
    "\n",
    "# system + user messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"<image>\\nYou are an InDrive car condition reviewer. \"\n",
    "                   \"Respond in a short, friendly sentence of up to 15 words. \"\n",
    "                   \"Only perfectly clean and undamaged cars are allowed. \"\n",
    "                   \"If the car is perfect, say something positive and end with 'Good for customers'. \"\n",
    "                   \"If the car has any issue, respond with 'Not acceptable (reason)', \"\n",
    "                   \"where reason is a brief description of the main problem (e.g., dirty, dent, rust). \"\n",
    "                   \"Always make the response polite and clear.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# build chat template\n",
    "text_prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# process inputs (directly pass images to processor)\n",
    "inputs = processor(\n",
    "    text=[text_prompt],\n",
    "    images=[image],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# generate output\n",
    "output_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "output_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
